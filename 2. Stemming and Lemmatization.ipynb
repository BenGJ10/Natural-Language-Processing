{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d598401-2f52-47a6-9a86-1a74f56e5540",
   "metadata": {},
   "source": [
    "## Stemming Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d035d21-b97f-4ccb-88c4-d6fde1c20e2a",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique that reduces words to their root or base form, also known as the stem. <br>\n",
    "It involves removing affixes (prefixes or suffixes) from words to simplify them for analysis and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c981aa58-c205-4a36-ab45-1cdb88ed9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "words  = [\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"written\", \"writter\", \"running\", \"runs\", \"runner\", \"history\", \"historian\", \"historical\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d94e16-d1bc-4b4d-b667-34601c501207",
   "metadata": {},
   "source": [
    "### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2602110c-4434-416c-8198-70dea0e00fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e380109b-15d6-4b18-a75c-6f972aa36f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "portstem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99074796-2643-4c0b-afa2-4244873cf8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eaten ----> eaten\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "written ----> written\n",
      "writter ----> writter\n",
      "running ----> run\n",
      "runs ----> run\n",
      "runner ----> runner\n",
      "history ----> histori\n",
      "historian ----> historian\n",
      "historical ----> histor\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" ----> \" + portstem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c4a77-9618-4507-84f3-573c1a429a43",
   "metadata": {},
   "source": [
    "For certain words, it literally changes the meaning of the word. This is one of the major disadvantages of stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a715aed-d912-4400-bff5-7253ccf36f73",
   "metadata": {},
   "source": [
    "### Regexp Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ae0d2-b7e8-4b60-9cdb-c5bd48cf795b",
   "metadata": {},
   "source": [
    "The Regexp Stemmer, or Regular Expression Stemmer, is a stemming algorithm that utilizes regular expressions to identify and remove suffixes from words. <br>\n",
    "It allows users to define custom rules for stemming by specifying patterns to match and remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f07fe397-a267-4b32-a77d-5057a05296a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e44727b-22cd-4290-9d7f-4c13b8b1ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexpstem = RegexpStemmer('ing$|s$|able$|e$', min = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa3e55bf-6c84-4b19-bc42-def3a287b3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexpstem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "929b6bbf-a7b5-4fca-9cc5-541c69b80b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eaten ----> eaten\n",
      "writing ----> writ\n",
      "writes ----> write\n",
      "written ----> written\n",
      "writter ----> writter\n",
      "running ----> runn\n",
      "runs ----> run\n",
      "runner ----> runner\n",
      "history ----> history\n",
      "historian ----> historian\n",
      "historical ----> historical\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" ----> \" + regexpstem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b84e1-da94-4340-b4a5-05e20faa0831",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529d581-7b8f-4e16-9faf-0469dc5b301a",
   "metadata": {},
   "source": [
    "The Snowball Stemmer, compared to the Porter Stemmer, is multi-lingual as it can handle non-English words. <br>\n",
    "The Snowball stemmer is way more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4fd20d3-ffd2-4099-8030-8b94be005531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd9d83c5-62ed-42cc-bd43-a7e5fbfd20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "560f24cd-89a0-43eb-ad65-7525a7bf6e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portstem.stem('fairly'), portstem.stem('sportingly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88650ce3-2212-448a-9f6f-9cb02a965ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstem.stem('fairly'), snowballstem.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85712e-ab08-472b-9459-bfe8068d6e37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3edf9-0814-46b3-b365-141f20ecafbc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b36c9f-49e5-4323-9c88-4299314c3d97",
   "metadata": {},
   "source": [
    "## Lemmatization Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a125ee8-622e-4cec-aa13-b3e3713d3e15",
   "metadata": {},
   "source": [
    "Lemmatization in Natural Language Processing (NLP) is a process that reduces words to their base or dictionary form, known as the lemma, while considering the word's context and meaning. Unlike stemming, which is a more rule-based approach, lemmatization analyzes word context to ensure that the correct dictionary form is selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64affa9d-e6d3-4f08-8636-5897bff7d5b1",
   "metadata": {},
   "source": [
    "### WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c4d3bdf-b51d-4447-8e23-3b016581d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb71d7b3-c993-444f-ba01-a41269ad123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnetlem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f1c8704-7689-48d8-a460-c3b5d2ec24e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS- \n",
    "Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''\n",
    "\n",
    "wordnetlem.lemmatize(\"going\", pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13f0e853-58e2-4340-b52b-b165fad7da55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "eats ----> eat\n",
      "eaten ----> eat\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "written ----> write\n",
      "writter ----> writter\n",
      "running ----> run\n",
      "runs ----> run\n",
      "runner ----> runner\n",
      "history ----> history\n",
      "historian ----> historian\n",
      "historical ----> historical\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word + \" ----> \" + wordnetlem.lemmatize(word, pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5b6e3-9c1c-4a29-8ecd-3534d739747c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
